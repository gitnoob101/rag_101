{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee1abe67-fc85-4e3f-a0b2-4748f790a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings  # or any embedding model of your choice\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# data_dir = \"./dataset\"\n",
    "\n",
    "\n",
    "# loader = DirectoryLoader(data_dir, glob=\"**/*.txt\", show_progress=True)\n",
    "# documents = loader.load()\n",
    "\n",
    "# for doc in documents[:2]:\n",
    "#     print(doc.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d37cdb-93ff-4393-9cc3-9db3e174f069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████▎         | 15/17 [00:06<00:00,  2.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "\n",
    "# Path to your research papers folder\n",
    "data_dir = \"./dataset\"\n",
    "\n",
    "# Use a glob pattern that matches PDF files and set the loader class to PyPDFLoader.\n",
    "loader = DirectoryLoader(\n",
    "    data_dir,\n",
    "    glob=\"**/*.pdf\",         # Match PDF files\n",
    "    loader_cls=PyPDFLoader,    # Use the PDF loader\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Load the documents\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1962d260-38db-4041-a2c8-22b6df3c421a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\\Non-Publishable\\R001.pdf\n",
      "dataset\\Non-Publishable\\R001.pdf\n",
      "dataset\\Non-Publishable\\R001.pdf\n",
      "dataset\\Non-Publishable\\R001.pdf\n",
      "dataset\\Non-Publishable\\R001.pdf\n",
      "dataset\\Non-Publishable\\R001.pdf\n",
      "dataset\\Non-Publishable\\R001.pdf\n",
      "dataset\\Non-Publishable\\R001.pdf\n",
      "dataset\\Non-Publishable\\R001.pdf\n",
      "dataset\\Non-Publishable\\R001.pdf\n",
      "dataset\\Non-Publishable\\R001.pdf\n",
      "dataset\\Non-Publishable\\R001.pdf\n",
      "dataset\\Non-Publishable\\R002.pdf\n",
      "dataset\\Non-Publishable\\R002.pdf\n",
      "dataset\\Non-Publishable\\R002.pdf\n",
      "dataset\\Non-Publishable\\R002.pdf\n",
      "dataset\\Non-Publishable\\R002.pdf\n",
      "dataset\\Non-Publishable\\R002.pdf\n",
      "dataset\\Non-Publishable\\R002.pdf\n",
      "dataset\\Non-Publishable\\R002.pdf\n",
      "dataset\\Non-Publishable\\R002.pdf\n",
      "dataset\\Non-Publishable\\R002.pdf\n",
      "dataset\\Non-Publishable\\R002.pdf\n",
      "dataset\\Non-Publishable\\R002.pdf\n",
      "dataset\\Non-Publishable\\R002.pdf\n",
      "dataset\\Non-Publishable\\R002.pdf\n",
      "dataset\\Non-Publishable\\R003.pdf\n",
      "dataset\\Non-Publishable\\R003.pdf\n",
      "dataset\\Non-Publishable\\R003.pdf\n",
      "dataset\\Non-Publishable\\R003.pdf\n",
      "dataset\\Non-Publishable\\R003.pdf\n",
      "dataset\\Non-Publishable\\R003.pdf\n",
      "dataset\\Non-Publishable\\R003.pdf\n",
      "dataset\\Non-Publishable\\R003.pdf\n",
      "dataset\\Non-Publishable\\R003.pdf\n",
      "dataset\\Non-Publishable\\R003.pdf\n",
      "dataset\\Non-Publishable\\R003.pdf\n",
      "dataset\\Non-Publishable\\R003.pdf\n",
      "dataset\\Non-Publishable\\R003.pdf\n",
      "dataset\\Non-Publishable\\R003.pdf\n",
      "dataset\\Non-Publishable\\R004.pdf\n",
      "dataset\\Non-Publishable\\R004.pdf\n",
      "dataset\\Non-Publishable\\R004.pdf\n",
      "dataset\\Non-Publishable\\R004.pdf\n",
      "dataset\\Non-Publishable\\R004.pdf\n",
      "dataset\\Non-Publishable\\R005.pdf\n",
      "dataset\\Non-Publishable\\R005.pdf\n",
      "dataset\\Non-Publishable\\R005.pdf\n",
      "dataset\\Non-Publishable\\R005.pdf\n",
      "dataset\\Non-Publishable\\R005.pdf\n",
      "dataset\\Non-Publishable\\R005.pdf\n",
      "dataset\\Non-Publishable\\R005.pdf\n",
      "dataset\\Non-Publishable\\R005.pdf\n",
      "dataset\\Non-Publishable\\R005.pdf\n",
      "dataset\\Non-Publishable\\R005.pdf\n",
      "dataset\\Non-Publishable\\R005.pdf\n",
      "dataset\\Publishable\\CVPR\\R006.pdf\n",
      "dataset\\Publishable\\CVPR\\R006.pdf\n",
      "dataset\\Publishable\\CVPR\\R006.pdf\n",
      "dataset\\Publishable\\CVPR\\R006.pdf\n",
      "dataset\\Publishable\\CVPR\\R006.pdf\n",
      "dataset\\Publishable\\CVPR\\R006.pdf\n",
      "dataset\\Publishable\\CVPR\\R006.pdf\n",
      "dataset\\Publishable\\CVPR\\R007.pdf\n",
      "dataset\\Publishable\\CVPR\\R007.pdf\n",
      "dataset\\Publishable\\CVPR\\R007.pdf\n",
      "dataset\\Publishable\\CVPR\\R007.pdf\n",
      "dataset\\Publishable\\CVPR\\R007.pdf\n",
      "dataset\\Publishable\\CVPR\\R007.pdf\n",
      "dataset\\Publishable\\CVPR\\R007.pdf\n",
      "dataset\\Publishable\\CVPR\\R007.pdf\n",
      "dataset\\Publishable\\CVPR\\R007.pdf\n",
      "dataset\\Publishable\\CVPR\\R007.pdf\n",
      "dataset\\Publishable\\EMNLP\\R008.pdf\n",
      "dataset\\Publishable\\EMNLP\\R008.pdf\n",
      "dataset\\Publishable\\EMNLP\\R008.pdf\n",
      "dataset\\Publishable\\EMNLP\\R008.pdf\n",
      "dataset\\Publishable\\EMNLP\\R008.pdf\n",
      "dataset\\Publishable\\EMNLP\\R008.pdf\n",
      "dataset\\Publishable\\EMNLP\\R008.pdf\n",
      "dataset\\Publishable\\EMNLP\\R008.pdf\n",
      "dataset\\Publishable\\EMNLP\\R008.pdf\n",
      "dataset\\Publishable\\EMNLP\\R009.pdf\n",
      "dataset\\Publishable\\EMNLP\\R009.pdf\n",
      "dataset\\Publishable\\EMNLP\\R009.pdf\n",
      "dataset\\Publishable\\EMNLP\\R009.pdf\n",
      "dataset\\Publishable\\EMNLP\\R009.pdf\n",
      "dataset\\Publishable\\EMNLP\\R009.pdf\n",
      "dataset\\Publishable\\EMNLP\\R009.pdf\n",
      "dataset\\Publishable\\KDD\\R010.pdf\n",
      "dataset\\Publishable\\KDD\\R010.pdf\n",
      "dataset\\Publishable\\KDD\\R010.pdf\n",
      "dataset\\Publishable\\KDD\\R010.pdf\n",
      "dataset\\Publishable\\KDD\\R010.pdf\n",
      "dataset\\Publishable\\KDD\\R010.pdf\n",
      "dataset\\Publishable\\KDD\\R010.pdf\n",
      "dataset\\Publishable\\KDD\\R010.pdf\n",
      "dataset\\Publishable\\KDD\\R010.pdf\n",
      "dataset\\Publishable\\KDD\\R010.pdf\n",
      "dataset\\Publishable\\KDD\\R011.pdf\n",
      "dataset\\Publishable\\KDD\\R011.pdf\n",
      "dataset\\Publishable\\KDD\\R011.pdf\n",
      "dataset\\Publishable\\KDD\\R011.pdf\n",
      "dataset\\Publishable\\KDD\\R011.pdf\n",
      "dataset\\Publishable\\KDD\\R011.pdf\n",
      "dataset\\Publishable\\KDD\\R011.pdf\n",
      "dataset\\Publishable\\KDD\\R011.pdf\n",
      "dataset\\Publishable\\NeurIPS\\R012.pdf\n",
      "dataset\\Publishable\\NeurIPS\\R012.pdf\n",
      "dataset\\Publishable\\NeurIPS\\R012.pdf\n",
      "dataset\\Publishable\\NeurIPS\\R012.pdf\n",
      "dataset\\Publishable\\NeurIPS\\R012.pdf\n",
      "dataset\\Publishable\\NeurIPS\\R012.pdf\n",
      "dataset\\Publishable\\NeurIPS\\R013.pdf\n",
      "dataset\\Publishable\\NeurIPS\\R013.pdf\n",
      "dataset\\Publishable\\NeurIPS\\R013.pdf\n",
      "dataset\\Publishable\\NeurIPS\\R013.pdf\n",
      "dataset\\Publishable\\NeurIPS\\R013.pdf\n",
      "dataset\\Publishable\\NeurIPS\\R013.pdf\n",
      "dataset\\Publishable\\NeurIPS\\R013.pdf\n",
      "dataset\\Publishable\\TMLR\\R014.pdf\n",
      "dataset\\Publishable\\TMLR\\R014.pdf\n",
      "dataset\\Publishable\\TMLR\\R014.pdf\n",
      "dataset\\Publishable\\TMLR\\R014.pdf\n",
      "dataset\\Publishable\\TMLR\\R014.pdf\n",
      "dataset\\Publishable\\TMLR\\R014.pdf\n",
      "dataset\\Publishable\\TMLR\\R014.pdf\n",
      "dataset\\Publishable\\TMLR\\R015.pdf\n",
      "dataset\\Publishable\\TMLR\\R015.pdf\n",
      "dataset\\Publishable\\TMLR\\R015.pdf\n",
      "dataset\\Publishable\\TMLR\\R015.pdf\n",
      "dataset\\Publishable\\TMLR\\R015.pdf\n",
      "dataset\\Publishable\\TMLR\\R015.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for doc in documents:\n",
    "  print(doc.metadata[\"source\"])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91aaffc0-e3f9-47e1-8645-d012ab56f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "# for i in docs:#just checking the split bitches\n",
    "#     print(i)\n",
    "    \n",
    "#     print(\"------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8086de87-ec11-437d-ac94-ab72ce9a4a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "model_name = \"all-MiniLM-L6-v2\"  \n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f36a59a-2051-4f0d-8621-46e8ac5dc5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3131d2fd-cfcf-4772-867a-e0927d4a4983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\\Publishable\\KDD\\R010.pdf\n",
      "----------------------------------------------------------------\n",
      "dataset\\Publishable\\KDD\\R010.pdf\n",
      "----------------------------------------------------------------\n",
      "dataset\\Publishable\\EMNLP\\R008.pdf\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pprint as p\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def classify_paper(file_path):\n",
    "    # Check the file extension and load content accordingly.\n",
    "    if file_path.lower().endswith(\".pdf\"):\n",
    "        # Use PyPDFLoader to load the PDF file.\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        # The loader returns a list of Document objects.\n",
    "        documents = loader.load()\n",
    "        # Concatenate the content from all pages/documents.\n",
    "        content = \"\\n\".join(doc.page_content for doc in documents)\n",
    "    else:\n",
    "        # Assume it's a text file.\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "    \n",
    "    # Build the query: include instructions and list of known organizations.\n",
    "   \n",
    "    query = content\n",
    "\n",
    "    # Run the chain – the retriever will add similar examples as context.\n",
    "  \n",
    "    return query\n",
    "\n",
    "# Example usage:\n",
    "new_paper_path = r\"C:\\Users\\avnex\\Downloads\\3534678.3539321.pdf\"\n",
    "query = classify_paper(new_paper_path)\n",
    "\n",
    "#########################################################################################################################################\n",
    "query_embedding = embeddings.embed_query(query)  # Convert query to vector\n",
    "similar_docs = vectorstore.similarity_search(query, k=3)  # Retrieve top 3 similar docs\n",
    "\n",
    "for doc in similar_docs:\n",
    "    \n",
    "    # print(doc.page_content)\n",
    "    print(doc.metadata[\"source\"])\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "# Create a list of formatted strings (page content + source)\n",
    "# formatted_docs = [f\"data: {doc.page_content}\\nSource: {doc.metadata['source']}\" for doc in similar_docs]\n",
    "# final_output = \"\\n\\n\".join(formatted_docs)\n",
    "# print(final_output)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e0f5c6b1-eeea-4363-9850-b4de988c67cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "llm = OllamaLLM(model=\"deepseek-r1:7b\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1d04841f-018d-4503-8079-2a016a0c1ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Classification: {'context': 'Data: Learning sub-intervals for pooling is found to be crucial for activity recognition.\\nMethod RGB Flow Two-stream\\nRandom 16.3 16.3 16.3\\nInceptionV3 + mean-pool 35.6 47.2 45.3\\nInceptionV3 + max-pool 47.9 48.6 54.4\\nInceptionV3 + pyramid 49.7 53.2 55.3\\nInceptionV3 + LSTM 47.6 55.6 57.7\\nInceptionV3 + temporal conv 47.2 55.2 56.1\\nInceptionV3 + sub-events 56.2 62.5 62.6\\nI3D + mean-pool 42.4 47.6 52.7\\nI3D + max-pool 48.3 53.4 57.2\\nI3D + pyramid 53.2 56.7 58.7\\nI3D + LSTM 48.2 53.1 53.1\\nI3D + temporal conv 52.8 57.1 58.4\\nI3D + sub-events 55.5 61.2 61.3\\nTable 5 shows the average precision for each activity class. Learning temporal structure is particularly\\nbeneficial for frame-based features (e.g., InceptionV3), which capture less temporal information\\n5\\nSource: dataset\\\\Publishable\\\\CVPR\\\\R006.pdf\\n\\nData: Next, the team selects keyframes. From the set {IDi}n\\ni=1, keyframes {IK\\nj }k\\nj=1 ⊆ {IDi}n\\ni=1 are\\nchosen. A method is implemented to detect and remove duplicate and blurry images, ensuring\\nhigh-quality frames. This involves applying a Gaussian blurring kernel followed by the fast Fourier\\ntransform method. Near-Image Similarity uses perceptual hashing and Hamming distance threshold-\\ning to detect similar images and retain overlapping ones. Duplicates and blurry images are excluded\\nto maintain data integrity and accuracy.\\nUsing the selected keyframes {IK\\nj }k\\nj=1, the team estimates camera poses through a method called\\nPixSfM, which involves extracting features using SuperPoint, matching them with SuperGlue, and\\nrefining them. The outputs are the camera poses {Cj}k\\nj=1, crucial for understanding the scene’s\\nspatial layout.\\n4\\nSource: dataset\\\\Publishable\\\\CVPR\\\\R007.pdf\\n\\nData: optimizer is ADAM, with a learning rate of 0.0003, a batch size of 216, and the number of training\\nepochs is 500.\\n6\\nSource: dataset\\\\Publishable\\\\NeurIPS\\\\R012.pdf', 'text': \"<think>\\nOkay, so I'm trying to figure out which of these three research papers is the most suitable for publication based on academic criteria. Let me go through each one step by step.\\n\\nStarting with Research Paper A: It's about the impact of renewable energy policies on carbon emissions reduction. The study uses panel data from 2015 to 2020, which seems like a good time frame. They used fixed effects and difference-in-differences models—those are solid econometric methods. However, I notice they only applied these to one country, which might limit the generalizability of their findings. Also, there's nothing about replicating the results or checking for robustness with other models like random effects or maybe even machine learning techniques. Maybe adding a control group from another region could strengthen it.\\n\\nMoving on to Research Paper B: This one looks at the effect of social media on mental health using a dataset from 2010-2023. They used OLS regression and found a significant negative correlation, but I'm not sure if that's enough proof. Also, they only included data from urban areas, which might bias their results because urban environments often have different social dynamics compared to rural or suburban areas. They don't discuss any potential confounders like socioeconomic status or access to mental health resources. Plus, the methodology section is a bit vague about how they collected the data—were these surveys conducted online? What about control variables?\\n\\nLastly, Research Paper C: It examines the impact of remote work on productivity and job satisfaction using a sample from 2021-2023. They used multiple regression analysis with dummy variables for different industries. The results are interesting because more than half the industries saw increased productivity, but they didn't control for factors like economic downturns or changes in workplace regulations that could have affected these outcomes. Also, there's no mention of replicating the study across different time periods or regions to ensure consistency.\\n\\nSo, considering all three, Research Paper A has strong methodological rigor with appropriate models and a relevant dataset but lacks cross-country analysis and robustness checks. Paper B addresses an important issue with some good data but lacks control for confounders and specific demographic factors. Paper C shows potential with multiple regression but misses out on controlling for external factors.\\n\\nI think the key differentiator is whether they've considered potential confounding variables, done replication or sensitivity analyses, and broadened their scope beyond just one country or region. Since none of them have done all these, maybe none meet the highest publishable standards, but perhaps Paper A has the strongest methodology given its limitations.\\n</think>\\n\\nBased on the analysis of the three research papers, none of them fully meet the stringent academic criteria for publication. Each paper has notable methodological gaps and could benefit from additional robustness checks.\\n\\n**Research Paper A:**\\n- **Strengths:** Uses appropriate econometric methods (fixed effects, difference-in-differences) with a relevant dataset.\\n- **Weaknesses:** Limited to one country, which reduces generalizability; lacks replication or alternative models for robustness.\\n\\n**Research Paper B:**\\n- **Strengths:** Addresses an important topic and provides some empirical evidence.\\n- **Weaknesses:** Data collected from urban areas may introduce bias; lacks control variables and detailed data collection methods.\\n\\n**Research Paper C:**\\n- **Strengths:** Demonstrates potential with multiple regression analysis.\\n- **Weaknesses:** No mention of controlling for external factors like economic downturns or regulatory changes.\\n\\nNone of the papers explicitly address the need for replicating studies across different regions/time periods, considering confounding variables, or conducting sensitivity analyses. While none meet all publishable standards, Research Paper A stands out as having the most robust methodology among the three due to its appropriate use of econometric techniques and focus on a critical issue, despite its limitations in generalizability.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load the LLM\n",
    "llm = OllamaLLM(model=\"deepseek-r1:7b\")\n",
    "\n",
    "# Combine retrieved documents into a single string with sources\n",
    "retrieved_texts = [\n",
    "    f\"Data: {doc.page_content}\\nSource: {doc.metadata['source']}\" for doc in similar_docs\n",
    "]\n",
    "retrieved_context = \"\\n\\n\".join(retrieved_texts)\n",
    "\n",
    "# Define the evaluation prompt\n",
    "evaluation_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template=(\n",
    "       \"you are a research paper analyst tasked with detereming wether a given research paper is publishable based on relevant dataset\"\n",
    "        \"the dataset consist of multiple context each containing data and methodological results and sources. the paper is categorized\"\n",
    "        \"under publishable and non-publishable based on standard acacdemic criteria.given the following three research paper snippiet anlyse\"\n",
    "        \"them carefully and determine whihc one is the most apt for publication.provide your final decision by selecting the most suitable research paper if none of them meets publishable standard state them expliciy\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the LLM chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=evaluation_prompt)\n",
    "\n",
    "# Run the classification\n",
    "result = llm_chain.invoke({\"context\": retrieved_context})\n",
    "\n",
    "# Extract and print only the final classification result\n",
    "print(\"Final Classification:\", result[\"text\"].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1866b44f-5e5e-4183-9505-334a42494221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Classification: <think>\n",
      "Okay, so I'm trying to figure out which of these three research papers is the most suitable for publication based on academic criteria. Let me go through each one step by step.\n",
      "\n",
      "Starting with Research Paper A: It's about the impact of renewable energy policies on carbon emissions reduction. The study uses panel data from 2015 to 2020, which seems like a good time frame. They used fixed effects and difference-in-differences models—those are solid econometric methods. However, I notice they only applied these to one country, which might limit the generalizability of their findings. Also, there's nothing about replicating the results or checking for robustness with other models like random effects or maybe even machine learning techniques. Maybe adding a control group from another region could strengthen it.\n",
      "\n",
      "Moving on to Research Paper B: This one looks at the effect of social media on mental health using a dataset from 2010-2023. They used OLS regression and found a significant negative correlation, but I'm not sure if that's enough proof. Also, they only included data from urban areas, which might bias their results because urban environments often have different social dynamics compared to rural or suburban areas. They don't discuss any potential confounders like socioeconomic status or access to mental health resources. Plus, the methodology section is a bit vague about how they collected the data—were these surveys conducted online? What about control variables?\n",
      "\n",
      "Lastly, Research Paper C: It examines the impact of remote work on productivity and job satisfaction using a sample from 2021-2023. They used multiple regression analysis with dummy variables for different industries. The results are interesting because more than half the industries saw increased productivity, but they didn't control for factors like economic downturns or changes in workplace regulations that could have affected these outcomes. Also, there's no mention of replicating the study across different time periods or regions to ensure consistency.\n",
      "\n",
      "So, considering all three, Research Paper A has strong methodological rigor with appropriate models and a relevant dataset but lacks cross-country analysis and robustness checks. Paper B addresses an important issue with some good data but lacks control for confounders and specific demographic factors. Paper C shows potential with multiple regression but misses out on controlling for external factors.\n",
      "\n",
      "I think the key differentiator is whether they've considered potential confounding variables, done replication or sensitivity analyses, and broadened their scope beyond just one country or region. Since none of them have done all these, maybe none meet the highest publishable standards, but perhaps Paper A has the strongest methodology given its limitations.\n",
      "</think>\n",
      "\n",
      "Based on the analysis of the three research papers, none of them fully meet the stringent academic criteria for publication. Each paper has notable methodological gaps and could benefit from additional robustness checks.\n",
      "\n",
      "**Research Paper A:**\n",
      "- **Strengths:** Uses appropriate econometric methods (fixed effects, difference-in-differences) with a relevant dataset.\n",
      "- **Weaknesses:** Limited to one country, which reduces generalizability; lacks replication or alternative models for robustness.\n",
      "\n",
      "**Research Paper B:**\n",
      "- **Strengths:** Addresses an important topic and provides some empirical evidence.\n",
      "- **Weaknesses:** Data collected from urban areas may introduce bias; lacks control variables and detailed data collection methods.\n",
      "\n",
      "**Research Paper C:**\n",
      "- **Strengths:** Demonstrates potential with multiple regression analysis.\n",
      "- **Weaknesses:** No mention of controlling for external factors like economic downturns or regulatory changes.\n",
      "\n",
      "None of the papers explicitly address the need for replicating studies across different regions/time periods, considering confounding variables, or conducting sensitivity analyses. While none meet all publishable standards, Research Paper A stands out as having the most robust methodology among the three due to its appropriate use of econometric techniques and focus on a critical issue, despite its limitations in generalizability.\n"
     ]
    }
   ],
   "source": [
    "# Extract and print only the final classification result\n",
    "print(\"Final Classification:\", result[\"text\"].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c8486-bc58-466e-ae0f-2f4640e3650e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc4c41-b2e0-49dd-8849-5073a6beb5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
